{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8199770,"sourceType":"datasetVersion","datasetId":4857428},{"sourceId":8200108,"sourceType":"datasetVersion","datasetId":4857661}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-23T04:32:26.772296Z","iopub.execute_input":"2024-04-23T04:32:26.772926Z","iopub.status.idle":"2024-04-23T04:32:27.831380Z","shell.execute_reply.started":"2024-04-23T04:32:26.772896Z","shell.execute_reply":"2024-04-23T04:32:27.830486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/dataset/Reviews.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install nltk","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch.optim as optim\nimport nltk\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T05:58:18.393301Z","iopub.execute_input":"2024-04-23T05:58:18.394193Z","iopub.status.idle":"2024-04-23T05:58:19.212959Z","shell.execute_reply.started":"2024-04-23T05:58:18.394160Z","shell.execute_reply":"2024-04-23T05:58:19.212201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['Text', 'Summary']]\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(df))\ndf.drop_duplicates(subset=['Text'],inplace=True) \nprint(len(df))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(axis=0, inplace=True)\nprint(df.isna().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contraction_mapping = {\n    \"ain't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'll\": \"he will\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'm\": \"I am\",\n    \"I've\": \"I have\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"this's\": \"this is\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"here's\": \"here is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T05:58:46.769815Z","iopub.execute_input":"2024-04-23T05:58:46.770803Z","iopub.status.idle":"2024-04-23T05:58:46.784299Z","shell.execute_reply.started":"2024-04-23T05:58:46.770767Z","shell.execute_reply":"2024-04-23T05:58:46.783419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ndef text_cleaner(text):\n    new_str = text.lower()\n    new_str = BeautifulSoup(new_str, \"lxml\").text\n    new_str = re.sub(r'\\([^)]*\\)', '', new_str)\n    new_str = re.sub('\"', '', new_str)\n    new_str = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_str.split(\" \")])\n    new_str = re.sub(r\"'s\\b\", \"\", new_str)\n    new_str = re.sub(\"[^a-zA-Z]\", \" \", new_str) \n    tokens = [w for w in new_str.split() if not w in stop_words]\n    long_words = []\n    for i in tokens:\n        if len(i) >= 3:  \n            long_words.append(i)\n    return (\" \".join(long_words)).strip() \n\n\ncleantxt = []\nfor t in df['Text']:\n    cleaned_text.append(text_cleaner(t))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def summary_cleaner(text):\n    new_str = re.sub('\"', '', str(text))\n    new_str = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_str.split(\" \")])  \n    new_str = re.sub(r\"'s\\b\", \"\", new_str)\n    new_str = re.sub(\"[^a-zA-Z]\", \" \", new_str)\n    new_str = new_str.lower()\n    tokens = new_str.split()\n    new_str = ''\n    for i in tokens:\n        if len(i) > 1:\n            new_str = new_str+i+' '\n    return new_str\n\n\ncleansum = []\nfor tmmm in df['Summary']:\n    cleaned_summary.append(summary_cleaner(tmmm))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['cleaned_text'] = cleantxt\ndf['cleaned_summary'] = cleansum\ndf['cleaned_summary'].replace('', np.nan, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(axis=0, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['Text','Summary','cleaned_text','cleaned_summary']]\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('yehbhi.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import FileLink\n\n\n# Create a download link for the CSV file\nFileLink(r'yehbhi.csv')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum_all_tokens = sum([len(review.split()) for review in df['cleaned_text']])\navg_length = sum_all_tokens / len(df['cleaned_text'])\navg_length","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum_all_tokens = sum([len(review.split()) for review in df['cleaned_summary']])\navg_length = sum_all_tokens / len(df['cleaned_summary'])\navg_length","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/finale/lpl.csv\")\ndf = df.sample(n=500, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:37:46.728996Z","iopub.execute_input":"2024-04-23T04:37:46.729921Z","iopub.status.idle":"2024-04-23T04:37:50.128782Z","shell.execute_reply.started":"2024-04-23T04:37:46.729875Z","shell.execute_reply":"2024-04-23T04:37:50.127905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df[['cleaned_text','cleaned_summary']]","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:37:50.130595Z","iopub.execute_input":"2024-04-23T04:37:50.130946Z","iopub.status.idle":"2024-04-23T04:37:50.140015Z","shell.execute_reply.started":"2024-04-23T04:37:50.130914Z","shell.execute_reply":"2024-04-23T04:37:50.139105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:37:50.141108Z","iopub.execute_input":"2024-04-23T04:37:50.142098Z","iopub.status.idle":"2024-04-23T04:37:50.162201Z","shell.execute_reply.started":"2024-04-23T04:37:50.142061Z","shell.execute_reply":"2024-04-23T04:37:50.161180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\nfrom torch.optim import Adam\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n\n\ntokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n                              \"bos_token\": \"<|startoftext|>\",\n                              \"eos_token\": \"<|endoftext|>\",\n                             })\n\nmodel.resize_token_embeddings(len(tokenizer))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:37:55.984124Z","iopub.execute_input":"2024-04-23T04:37:55.984803Z","iopub.status.idle":"2024-04-23T04:37:56.696767Z","shell.execute_reply.started":"2024-04-23T04:37:55.984768Z","shell.execute_reply":"2024-04-23T04:37:56.695846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_df, test_df = train_test_split(df, test_size=0.25, random_state=42)\n\nclass SummaryDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        self.data = df\n        self.texts = self.data['cleaned_text'].values\n        self.summaries = self.data['cleaned_summary'].values\n\n        self.X = []\n\n        for text, summary in zip(self.texts, self.summaries):\n            self.X.append(text + \"  \" + summary + \" \")\n\n        self.X_encoded = tokenizer(self.X, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n        self.input_ids = self.X_encoded['input_ids']\n        self.attention_mask = self.X_encoded['attention_mask']\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return (self.input_ids[idx], self.attention_mask[idx])\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token \n\ntrain_dataset = SummaryDataset(train_df, tokenizer)\ntest_dataset = SummaryDataset(test_df, tokenizer)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:38:00.863787Z","iopub.execute_input":"2024-04-23T04:38:00.864761Z","iopub.status.idle":"2024-04-23T04:38:01.801002Z","shell.execute_reply.started":"2024-04-23T04:38:00.864727Z","shell.execute_reply":"2024-04-23T04:38:01.800208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset.__getitem__(0)[0])","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:38:04.278677Z","iopub.execute_input":"2024-04-23T04:38:04.279007Z","iopub.status.idle":"2024-04-23T04:38:04.285202Z","shell.execute_reply.started":"2024-04-23T04:38:04.278981Z","shell.execute_reply":"2024-04-23T04:38:04.284215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cpu\")\ndevice\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Assuming you have defined your model and tokenizer elsewhere in your code\n# Also assuming you have already defined train_dataloader and test_dataloader\n\ndevice = torch.device(\"cpu\")\n\noptim = Adam(model.parameters(), lr=1e-3)\n\ndef train(dataloader, model, optim, device):\n    epochs = 1\n    losses = []\n\n    model.to(device)  # Move model to CPU device\n\n    for epoch in range(epochs):\n        print(f'Training - Epoch: {epoch + 1} started')\n        total_loss = 0\n        num_batches = 0\n\n        for input_ids, attention_mask in tqdm(dataloader):\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n\n            optim.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n            loss = outputs.loss\n            loss.backward()\n            optim.step()\n\n            total_loss += loss.item()\n            num_batches += 1\n            losses.append(loss.item())\n\n        avg_loss = total_loss / num_batches\n        print(f'Training - Epoch: {epoch + 1}, Loss: {avg_loss:.4f}')\n\n        if avg_loss <= 1.5:\n            model.save_pretrained('gpt2_024')\n            torch.save(model.state_dict(), 'gpt2_024.pth')\n            break\n\n    return losses\n\ndef evaluate(dataloader, model, device):\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(dataloader):\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n            loss = outputs.loss\n\n            total_loss += loss.item()\n            num_batches += 1\n\n    avg_loss = total_loss / num_batches\n    print(f'Testing - Loss: {avg_loss:.4f}')\n    return avg_loss\n\ntrain_losses = train(train_dataloader, model, optim, device)\ntest_loss = evaluate(test_dataloader, model, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:39:28.585980Z","iopub.execute_input":"2024-04-23T04:39:28.586660Z","iopub.status.idle":"2024-04-23T05:34:35.948640Z","shell.execute_reply.started":"2024-04-23T04:39:28.586616Z","shell.execute_reply":"2024-04-23T05:34:35.947658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load the fine-tuned model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained('gpt2_024')\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2_024')\n\n# Set device\ndevice = torch.device(\"cpu\")  # or \"cuda\" if you have GPU\n\n# Define a function to generate summaries\ndef generate_summary(review_text, model, tokenizer, device):\n    # Clean the review text\n    cleaned_review = text_cleaner(review_text)\n    \n    # Tokenize the cleaned review text\n    inputs = tokenizer(cleaned_review, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\").to(device)\n    \n    # Generate summary using the model\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=100, num_beams=4, early_stopping=True)\n    \n    # Decode the generated summary\n    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return generated_summary\n\n# Example review text and summary\ns=input()\nreview_text=s\n# review_summary = \"Amazing product, exceeded expectations.\"\n\n# Generate summary for the review text using the fine-tuned model\ngenerated_summary = generate_summary(review_text, model, tokenizer, device)\n\n# Print the generated summary\nprint(\"Generated Summary:\", generated_summary)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T05:45:38.562761Z","iopub.execute_input":"2024-04-23T05:45:38.563510Z","iopub.status.idle":"2024-04-23T05:45:38.568383Z","shell.execute_reply.started":"2024-04-23T05:45:38.563479Z","shell.execute_reply":"2024-04-23T05:45:38.567391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaner(text):\n    new_str = text.lower()\n    new_str = BeautifulSoup(new_str, \"lxml\").text\n    new_str = re.sub(r'\\([^)]*\\)', '', new_str)\n    new_str = re.sub('\"', '', new_str)\n    new_str = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_str.split(\" \")])\n    new_str = re.sub(r\"'s\\b\", \"\", new_str)\n    new_str = re.sub(\"[^a-zA-Z]\", \" \", new_str) \n    tokens = [w for w in new_str.split() if not w in stop_words]\n    long_words = []\n    for i in tokens:\n        if len(i) >= 3:  \n            long_words.append(i)\n    return (\" \".join(long_words)).strip() ","metadata":{"execution":{"iopub.status.busy":"2024-04-23T05:57:52.585167Z","iopub.execute_input":"2024-04-23T05:57:52.585991Z","iopub.status.idle":"2024-04-23T05:57:52.592952Z","shell.execute_reply.started":"2024-04-23T05:57:52.585961Z","shell.execute_reply":"2024-04-23T05:57:52.592053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T06:11:07.754294Z","iopub.execute_input":"2024-04-23T06:11:07.754945Z","iopub.status.idle":"2024-04-23T06:11:07.869467Z","shell.execute_reply.started":"2024-04-23T06:11:07.754911Z","shell.execute_reply":"2024-04-23T06:11:07.868596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to generate summaries\ndef generate_summary(review_text, model, tokenizer, device):\n    # Clean the review text\n    cleaned_review = text_cleaner(review_text)\n    \n    # Tokenize the cleaned review text\n    inputs = tokenizer(cleaned_review, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\").to(device)\n    \n    # Generate summary using the model\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=1024, num_beams=4, early_stopping=True)\n    \n    # Decode the generated summary\n    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    return generated_summary\n\n\n# Example review text and summary\ns=input()\nreview_text=s\n# review_summary = \"Amazing product, exceeded expectations.\"\n\n# Generate summary for the review text using the fine-tuned model\ngenerated_summary = generate_summary(review_text, model, tokenizer, device)\n\n# Print the generated summary\nprint(\"Generated Summary:\", generated_summary)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T06:12:57.687102Z","iopub.execute_input":"2024-04-23T06:12:57.687607Z","iopub.status.idle":"2024-04-23T06:12:57.692802Z","shell.execute_reply.started":"2024-04-23T06:12:57.687571Z","shell.execute_reply":"2024-04-23T06:12:57.691679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install rouge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\n\n# Initialize Rouge\nrouge = Rouge()\n# Compute ROUGE scores\nscores = rouge.get_scores(generated_summary, actual_summary)\n\n# Print ROUGE scores\nprint(\"ROUGE-1: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(scores[0]['rouge-1']['p'], scores[0]['rouge-1']['r'], scores[0]['rouge-1']['f']))\nprint(\"ROUGE-2: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(scores[0]['rouge-2']['p'], scores[0]['rouge-2']['r'], scores[0]['rouge-2']['f']))\nprint(\"ROUGE-L: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(scores[0]['rouge-l']['p'], scores[0]['rouge-l']['r'], scores[0]['rouge-l']['f']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###pls note my kaggle went out of memory and kaggle notebook restarted therfore lost all the outputs\n###I am trying to run on a very small dataset with a single epoch and 500 split data for output showing however during demo i will train it again as done before the genuine out of memory issue.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n\n# Step 1: Initialize a GPT-2 tokenizer and model from Hugging Face\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.add_special_tokens({'pad_token': ''})  # Add padding token\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Step 2: Load and split the dataset\ndata = df  # Replace \"your_dataset.csv\" with the path to your dataset\ntrain_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n\n# Step 3: Implement a custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['cleaned_text']\n        summary = self.data.iloc[idx]['cleaned_summary']\n        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n        labels = self.tokenizer.encode_plus(summary, add_special_tokens=True, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'labels': labels['input_ids'].flatten(),\n        }\n\n# Step 4: Fine-tune the GPT-2 model on the review dataset to generate summaries\ntraining_args = TrainingArguments(\n    output_dir='./output',  # Specify the output directory\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    logging_dir='./logs',\n)\n\ntrain_dataset = CustomDataset(train_data, tokenizer, max_length=512)\ntest_dataset = CustomDataset(test_data, tokenizer, max_length=512)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\ntrainer.train()\n\n# Define the directory path to save the fine-tuned model\noutput_directory = \"./fine_tuned_gpt2_model\"\n\n# Save the fine-tuned model and tokenizer\nmodel.save_pretrained(output_directory)\ntokenizer.save_pretrained(output_directory)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Define the directory path where the fine-tuned model and tokenizer are saved\noutput_directory = \"./fine_tuned_gpt2_model\"\n\n# Load the fine-tuned model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(output_directory)\ntokenizer = GPT2Tokenizer.from_pretrained(output_directory)\n\n# Define the device (either cuda:0 for GPU or cpu for CPU)\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Move the model to the specified device\nmodel.to(device)\n\n# Step 1: Preprocess the input review text using the tokenizer\ninput_review_text = \"The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound, and feels comfortable to play. However, some users have reported issues with the tuning stability.\"\ninputs = tokenizer(input_review_text, return_tensors=\"pt\")\n\n# Move input tensors to the same device as the model\ninputs = {key: value.to(device) for key, value in inputs.items()}\n\n# Step 2: Pass the preprocessed input review text to the fine-tuned model to generate the summary\noutputs = model.generate(input_ids=inputs['input_ids'], max_length=50, num_beams=4, early_stopping=True)\n\n# Step 3: Decode the generated summary using the tokenizer\ngenerated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Generated Summary:\", generated_summary)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound, and feels comfortable to play. However, some users have reported issues with the tuning stability.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load the fine-tuned model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained(\"./output\")  # Load from the directory where the model was saved during fine-tuning\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Define the device (either cuda:0 for GPU or cpu for CPU)\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Move the model to the specified device\nmodel.to(device)\n\n# Step 1: Preprocess the input review text using the tokenizer\ninput_review_text = \"The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound, and feels comfortable to play. However, some users have reported issues with the tuning stability.\"\ninputs = tokenizer(input_review_text, return_tensors=\"pt\")\n\n# Move input tensors to the same device as the model\ninputs = {key: value.to(device) for key, value in inputs.items()}\n\n# Step 2: Pass the preprocessed input review text to the fine-tuned model to generate the summary\noutputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=50, num_beams=4, early_stopping=True)\n\n# Step 3: Decode the generated summary using the tokenizer\ngenerated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"Generated Summary:\", generated_summary)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}